{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feff170a"
   },
   "source": [
    "# Índice\n",
    "\n",
    "1. [Importação de dados e de bibliotecas; separação de cada batch ](#Importação-de-dados-e-de-bibliotecas;-separação-de-cada-batch )\n",
    "2. [Tipos de controle](#Tipos-de-controle)\n",
    "3. [Estudo batches 0-29: Controlled by recipe driven approach](#Estudo-batches-0-29:-Controlled-by-recipe-driven-approach)\n",
    "4. [Estudo batches 60-89: Controlled by an Advanced Process Control (APC)](#Estudo-batches-60-89:-Controlled-by-an-Advanced-Process-Control-(APC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37475a6d"
   },
   "source": [
    "# Importação de dados e de bibliotecas; separação de cada batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18c0b2f6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import iplot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, LSTM, Dense, RNN, GRU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "import time\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.rnn import ESNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fcf18d1",
    "outputId": "2cdfe1eb-5848-4054-80b4-2a243c16e332",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"100_Batches_IndPenSim_V3.csv\")\n",
    "\n",
    "# ou: data = pd.read_csv(r\"Especifique aqui o caminho para o arquivo .csv que contém os dados referentes às bateladas simuladas no IndPenSim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "244ff05f"
   },
   "outputs": [],
   "source": [
    "l = data[data['Time (h)']==0.2].index.to_list()\n",
    "batches_list=[]\n",
    "for idx in range(0, len(l)):\n",
    "    if idx+1 <= len(l)-1:\n",
    "        batches_list.append(data.loc[l[idx]:(l[idx+1]-1)])\n",
    "    else:\n",
    "        batches_list.append(data.loc[l[idx]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d384e98b"
   },
   "source": [
    "Queremos que todos os batches utilizados na rede neural tenham a mesma duração de 230 horas (é a duração dos batches 29, 17, 5, ...). Os batches com duração menor que essa serão descartados, e, para os batches com duração maior, apenas os dados até esta hora serão considerados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9aa273b4",
    "outputId": "17ff0ecd-aeff-48a5-8c8b-db1668bd2813"
   },
   "outputs": [],
   "source": [
    "duracao_minima = batches_list[29].shape[0]\n",
    "duracao_minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c342cfab",
    "outputId": "1d615f0a-d96c-41cb-8fc3-f3b67e03ee29"
   },
   "outputs": [],
   "source": [
    "batches_utilizados = []\n",
    "\n",
    "for i in range(0,30):\n",
    "    if batches_list[i].shape[0] >= duracao_minima:\n",
    "        batches_utilizados.append(i)\n",
    "        batches_list[i] = batches_list[i][:duracao_minima]  # Atualizamos os dados para estes batches em batches_list\n",
    "\n",
    "batches_utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40d4c548"
   },
   "outputs": [],
   "source": [
    "colunas_utilizaveis = [\n",
    " 'Time (h)',\n",
    " 'Aeration rate(Fg:L/h)',\n",
    " 'Agitator RPM(RPM:RPM)',\n",
    " 'Sugar feed rate(Fs:L/h)',\n",
    " 'Acid flow rate(Fa:L/h)',\n",
    " 'Base flow rate(Fb:L/h)',\n",
    " 'Heating/cooling water flow rate(Fc:L/h)',\n",
    " 'Heating water flow rate(Fh:L/h)',\n",
    " 'Water for injection/dilution(Fw:L/h)',\n",
    " 'Air head pressure(pressure:bar)',\n",
    " 'Dumped broth flow(Fremoved:L/h)',\n",
    " 'Substrate concentration(S:g/L)',\n",
    " 'Dissolved oxygen concentration(DO2:mg/L)',\n",
    " 'Penicillin concentration(P:g/L)',\n",
    " 'Vessel Volume(V:L)',\n",
    " 'Vessel Weight(Wt:Kg)',\n",
    " 'pH(pH:pH)',\n",
    " 'Temperature(T:K)',\n",
    " 'Generated heat(Q:kJ)',\n",
    " 'carbon dioxide percent in off-gas(CO2outgas:%)',\n",
    " 'PAA flow(Fpaa:PAA flow (L/h))',\n",
    " 'PAA concentration offline(PAA_offline:PAA (g L^{-1}))',\n",
    " 'Oil flow(Foil:L/hr)',\n",
    " 'NH_3 concentration off-line(NH3_offline:NH3 (g L^{-1}))',\n",
    " 'Oxygen Uptake Rate(OUR:(g min^{-1}))',\n",
    " 'Oxygen in percent in off-gas(O2:O2  (%))',\n",
    " 'Offline Penicillin concentration(P_offline:P(g L^{-1}))',\n",
    " 'Offline Biomass concentratio(X_offline:X(g L^{-1}))',\n",
    " 'Carbon evolution rate(CER:g/h)',\n",
    " 'Ammonia shots(NH3_shots:kgs)',\n",
    " 'Viscosity(Viscosity_offline:centPoise)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06895e5e"
   },
   "source": [
    "Os batches que fornecem maiores valores finais de conc. de penicilina são os batches 7 e 28. Os que fornecem menores conc. são 8 e 29. Queremos que estes batches sejam utilizados para treinamento do modelo RNN. Os batches intermediários serão separados em treino e teste.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e00be171",
    "outputId": "2cafa320-5078-45bf-a858-7aa561ecebfa"
   },
   "outputs": [],
   "source": [
    "ultimos_valores_penic_batches_utilizados = []\n",
    "for i in batches_utilizados:\n",
    "    ultimos_valores_penic_batches_utilizados.append(batches_list[i]['Penicillin concentration(P:g/L)'].to_list()[-1])\n",
    "\n",
    "ultimos_valores_penic_batches_utilizados = pd.DataFrame(ultimos_valores_penic_batches_utilizados, batches_utilizados)\n",
    "ultimos_valores_penic_batches_utilizados = ultimos_valores_penic_batches_utilizados.sort_values(by = 0, ascending = False)\n",
    "\n",
    "batches_maior_penic = ultimos_valores_penic_batches_utilizados.index[[0,1]].to_list()\n",
    "batches_menor_penic = ultimos_valores_penic_batches_utilizados.index[[-1,-2]].to_list()\n",
    "print(batches_maior_penic)\n",
    "print(batches_menor_penic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36a8f2a2",
    "outputId": "d0fe420c-f067-4589-a052-d855fa3b964b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "rng = np.random.RandomState(42)\n",
    "batches_utilizados = np.array(batches_utilizados)\n",
    "\n",
    "proporcao_treino = 0.75\n",
    "batches_garantidos_no_treino = np.array(batches_maior_penic + batches_menor_penic)\n",
    "\n",
    "qntd_randomizada_treino = int(  (len(batches_utilizados) * proporcao_treino - len(batches_garantidos_no_treino) )  )\n",
    "                              \n",
    "batches_randomizados_treino = rng.choice(np.setxor1d(batches_utilizados, batches_garantidos_no_treino), qntd_randomizada_treino, replace=False) \n",
    "# A função np.setxor1d retorna um array com os valores que encontram-se APENAS no primeiro array fornecido. Ou seja, pega o \n",
    "# primeiro array e retira os valores que estão no segundo array. https://numpy.org/doc/stable/reference/generated/numpy.setxor1d.html\n",
    "\n",
    "batches_treino = np.concatenate((batches_randomizados_treino, batches_garantidos_no_treino))\n",
    "\n",
    "print(len(batches_treino))\n",
    "batches_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61f96ec8",
    "outputId": "717063bb-2a17-4bd9-842a-bc79c1d06fd8"
   },
   "outputs": [],
   "source": [
    "batches_teste = np.setxor1d(batches_utilizados, batches_treino)\n",
    "print(len(batches_teste))\n",
    "batches_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e370ae96",
    "outputId": "1e3db5b2-fb2f-499f-9a13-4186851daacc"
   },
   "outputs": [],
   "source": [
    "batches_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a52be578",
    "outputId": "d6952c4f-0dcc-4d7e-df54-daef08f484e5"
   },
   "outputs": [],
   "source": [
    "batches_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1c3fa69",
    "outputId": "af8df705-fff5-4fab-ab46-3608fc3a3611"
   },
   "outputs": [],
   "source": [
    "batches_utilizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodologia utilizada para se treinar a rede neural foi inspirada nos conteúdos do capítulo 5 do livro Neural Networks \n",
    "in Bioprocessing and Chemical Engineering, de D. R. Baughman e Y. A. Liu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1b8da71"
   },
   "outputs": [],
   "source": [
    "colunas_input = ['Carbon evolution rate(CER:g/h)',\n",
    "                 'Oxygen Uptake Rate(OUR:(g min^{-1}))', \n",
    "                 'Substrate concentration(S:g/L)', \n",
    "                 'Penicillin concentration(P:g/L)' ]\n",
    "\n",
    "qntd_features_input = len(colunas_input)\n",
    "\n",
    "colunas_output = ['Penicillin concentration(P:g/L)']\n",
    "\n",
    "colunas_utilizadas = colunas_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44fd5d71",
    "outputId": "83f03c49-cbd1-4b7e-b168-8e123e7a89ac"
   },
   "outputs": [],
   "source": [
    "batches_list[3][colunas_utilizadas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65b43b96"
   },
   "outputs": [],
   "source": [
    "for i in batches_utilizados:\n",
    "    globals()['dados_batch_'+str(i)] = globals()['batches_list'][i].copy().reset_index()[colunas_utilizadas]\n",
    "\n",
    "# Para fazer o StandardScaling, preciso juntar todos os dados de treino em uma só tabela. Assim, farei o fit para essa tabelona\n",
    "# e, depois, o transform para todas as tabelas (de treino e de teste) em separado.\n",
    "\n",
    "# FAZENDO O FIT:\n",
    "\n",
    "passador = 1\n",
    "\n",
    "for i in batches_treino:\n",
    "    \n",
    "    if passador == 2:\n",
    "    \n",
    "        df_1 = globals()['dados_batch_%i' %i]\n",
    "        df_2 = globals()['dados_batch_%i' %i_anterior]\n",
    "        \n",
    "        todos_batches_treino = pd.concat([df_1, df_2], sort=False)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if passador > 2:\n",
    "            \n",
    "            df = globals()['dados_batch_%i' %i]\n",
    "            todos_batches_treino = pd.concat([todos_batches_treino, df], sort=False)\n",
    "            \n",
    "    i_anterior = i                                                                                                                                                                                     \n",
    "    passador += 1\n",
    "\n",
    "colunas = todos_batches_treino.columns\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "todos_batches_treino_scaled = scaler.fit(todos_batches_treino)\n",
    "\n",
    "\n",
    "# FAZENDO O TRANSFORM PARA CADA BATCH (TANTO TREINO QUANTO TESTE):\n",
    "\n",
    "for i in batches_utilizados:\n",
    "    temporario = globals()['dados_batch_'+str(i)].copy()\n",
    "    escalado = pd.DataFrame(scaler.transform(temporario), columns = colunas).copy()\n",
    "    \n",
    "    globals()['dados_batch_'+str(i)] = escalado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e5cadf0",
    "outputId": "b245480b-9c89-422c-d9cf-5b0a4b27a255"
   },
   "outputs": [],
   "source": [
    "len_batches = globals()['dados_batch_' + str(batches_treino[0])].shape[0]\n",
    "len_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8be9a27"
   },
   "outputs": [],
   "source": [
    "# Criando os objetos X (input) e Y (output) para o treino:\n",
    "\n",
    "X_treino = []\n",
    "Y_treino = []\n",
    "\n",
    "# Especifique o tamanho da janela de treino:\n",
    "T = 20\n",
    "\n",
    "for i in batches_treino:\n",
    "    \n",
    "    globals()['X_batch_'+str(i)] = globals()['dados_batch_' + str(i)][colunas_input].values\n",
    "\n",
    "    \n",
    "    globals()['Y_batch_'+str(i)] = globals()['dados_batch_' + str(i)][colunas_output].values\n",
    "    \n",
    "\n",
    "    for t in range(len_batches-T):\n",
    "        x = globals()['X_batch_'+str(i)][t:t+T]\n",
    "        X_treino.append(x)\n",
    "\n",
    "        y =  globals()['Y_batch_'+str(i)][t+T]\n",
    "        Y_treino.append(y)\n",
    "\n",
    "# Observe que X é uma 'tabelona' com TODAS as janelas de TODOS os batches, e Y é o output de cada uma destas janelas.\n",
    "np.array(X_treino)\n",
    "X_treino = np.array(X_treino).reshape(-1,T,qntd_features_input)\n",
    "Y_treino = np.array(Y_treino).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4a14ef2"
   },
   "outputs": [],
   "source": [
    "# Criando os objetos X (input) e Y (output) para o teste:\n",
    "\n",
    "X_teste = []\n",
    "Y_teste = []\n",
    "\n",
    "for i in batches_teste:\n",
    "    \n",
    "    globals()['X_batch_'+str(i)] = globals()['dados_batch_' + str(i)][colunas_input].values\n",
    "\n",
    "    \n",
    "    globals()['Y_batch_'+str(i)] = globals()['dados_batch_' + str(i)][colunas_output].values\n",
    "    \n",
    "\n",
    "    for t in range(len_batches-T):\n",
    "        x = globals()['X_batch_'+str(i)][t:t+T]\n",
    "        X_teste.append(x)\n",
    "\n",
    "        y =  globals()['Y_batch_'+str(i)][t+T]\n",
    "        Y_teste.append(y)\n",
    "\n",
    "X_teste = np.array(X_teste).reshape(-1,T,qntd_features_input)\n",
    "Y_teste = np.array(Y_teste).reshape(-1,1)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNÇÃO GENERALIZADORA DE MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelo_generalizado(primeira_camada, segunda_camada, terceira_camada, numero_neuronios, T, qntd_features_input):\n",
    "        \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    i = Input(shape = (T,qntd_features_input))\n",
    "    \n",
    "    numero_camadas = 1\n",
    "    proporções = [] \n",
    "    \n",
    "    if(primeira_camada == '0'):\n",
    "    \n",
    "        return('Erro: primeira camada inexistente')\n",
    "    \n",
    "    if(terceira_camada == '0'):\n",
    "    \n",
    "        if(segunda_camada == '0'):\n",
    "    \n",
    "            numero_camadas = 1\n",
    "    \n",
    "        else :\n",
    "            numero_camadas = 2\n",
    "    else: \n",
    "        numero_camadas = 3\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    #Caso tenhamos tres camadas    \n",
    "    \n",
    "    if(numero_camadas == 3):\n",
    "        proporcoes = [3, 2, 1]\n",
    "        \n",
    "        #Definindo a primeira camada\n",
    "        if(primeira_camada == 'D'):\n",
    "        \n",
    "            x = Dense(numero_neuronios*proporcoes[0]//sum(proporcoes))(i)\n",
    "    \n",
    "        elif(primeira_camada == 'S'): \n",
    "            \n",
    "            x = SimpleRNN(numero_neuronios*proporcoes[0]//sum(proporcoes), return_sequences = True)(i)\n",
    "\n",
    "        elif(primeira_camada == 'L'): \n",
    "            \n",
    "            x = LSTM(numero_neuronios*proporcoes[0]//sum(proporcoes), return_sequences = True)(i)\n",
    "     \n",
    "        elif(primeira_camada == 'G'): \n",
    "            \n",
    "            x = GRU(numero_neuronios*proporcoes[0]//sum(proporcoes), return_sequences = True)(i)\n",
    "    \n",
    "        elif(primeira_camada == 'E'): \n",
    "            \n",
    "            x = RNN(ESNCell(numero_neuronios*proporcoes[0]//sum(proporcoes)), return_sequences = True)(i)\n",
    "        \n",
    "        \n",
    "        #Definindo a segunda camada\n",
    "        \n",
    "        if(segunda_camada == 'D'):\n",
    "        \n",
    "            x = Dense(numero_neuronios*proporcoes[1]//sum(proporcoes))(x)\n",
    "        \n",
    "        elif(segunda_camada == 'S'): \n",
    "        \n",
    "            x = SimpleRNN(numero_neuronios*proporcoes[1]//sum(proporcoes), return_sequences = True)(x)\n",
    "        \n",
    "        elif(segunda_camada == 'L'): \n",
    "            \n",
    "            x = LSTM(numero_neuronios*proporcoes[1]//sum(proporcoes), return_sequences = True)(x)\n",
    "        \n",
    "        elif(segunda_camada == 'G'): \n",
    "            \n",
    "            x = GRU(numero_neuronios*proporcoes[1]//sum(proporcoes), return_sequences = True)(x)\n",
    "    \n",
    "        elif(segunda_camada == 'E'): \n",
    "            \n",
    "            x = RNN(ESNCell(numero_neuronios*proporcoes[1]//sum(proporcoes)), return_sequences = True)(x)\n",
    "    \n",
    "    \n",
    "        #Definindo a terceira camada\n",
    "        if(terceira_camada == 'D'):\n",
    "            \n",
    "            x = Dense(numero_neuronios*proporcoes[2]//sum(proporcoes))(x)\n",
    "    \n",
    "        elif(terceira_camada == 'S'): \n",
    "            \n",
    "            x = SimpleRNN(numero_neuronios*proporcoes[2]//sum(proporcoes))(x)\n",
    "\n",
    "        elif(terceira_camada == 'L'): \n",
    "            \n",
    "            x = LSTM(numero_neuronios*proporcoes[2]//sum(proporcoes))(x)\n",
    "     \n",
    "        elif(terceira_camada == 'G'): \n",
    "            \n",
    "            x = GRU(numero_neuronios*proporcoes[2]//sum(proporcoes))(x)\n",
    "    \n",
    "        elif(terceira_camada == 'E'): \n",
    "            \n",
    "            x = RNN(ESNCell(numero_neuronios*proporcoes[2]//sum(proporcoes)))(x)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    elif(numero_camadas == 2):\n",
    "        proporcoes = [2, 1]\n",
    "        \n",
    "        if(primeira_camada == 'D'):\n",
    "            \n",
    "            x = Dense(numero_neuronios*proporcoes[0]//sum(proporcoes))(i)\n",
    "    \n",
    "        elif(primeira_camada == 'S'): \n",
    "            \n",
    "            x = SimpleRNN(numero_neuronios*proporcoes[0]//sum(proporcoes), return_sequences = True)(i)\n",
    "\n",
    "        elif(primeira_camada == 'L'): \n",
    "            \n",
    "            x = LSTM(numero_neuronios*proporcoes[0]//sum(proporcoes), return_sequences = True)(i)\n",
    "     \n",
    "        elif(primeira_camada == 'G'): \n",
    "            \n",
    "            x = GRU(numero_neuronios*proporcoes[0]//sum(proporcoes), return_sequences = True)(i)\n",
    "    \n",
    "        elif(primeira_camada == 'E'): \n",
    "            \n",
    "            x = RNN(ESNCell(numero_neuronios*proporcoes[0]//sum(proporcoes)), return_sequences = True)(i)\n",
    "        \n",
    "        #Definindo a segunda camada\n",
    "        if(segunda_camada == 'D'):\n",
    "            \n",
    "            x = Dense(numero_neuronios*proporcoes[1]//sum(proporcoes))(x)\n",
    "    \n",
    "        elif(segunda_camada == 'S'): \n",
    "            \n",
    "            x = SimpleRNN(numero_neuronios*proporcoes[1]//sum(proporcoes))(x)\n",
    "\n",
    "        elif(segunda_camada == 'L'): \n",
    "            \n",
    "            x = LSTM(numero_neuronios*proporcoes[1]//sum(proporcoes))(x)\n",
    "     \n",
    "        elif(segunda_camada == 'G'): \n",
    "            \n",
    "            x = GRU(numero_neuronios*proporcoes[1]//sum(proporcoes))(x)\n",
    "    \n",
    "        elif(segunda_camada == 'E'): \n",
    "            \n",
    "            x = RNN(ESNCell(numero_neuronios*proporcoes[1]//sum(proporcoes)))(x)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    if(numero_camadas == 1):\n",
    "        proporcoes = [1]\n",
    "    \n",
    "        if(primeira_camada == 'D'):\n",
    "            \n",
    "            x = Dense(numero_neuronios*proporcoes[0]//sum(proporcoes))(i)\n",
    "    \n",
    "        elif(primeira_camada == 'S'): \n",
    "            \n",
    "            x = SimpleRNN(numero_neuronios*proporcoes[0]//sum(proporcoes))(i)\n",
    "\n",
    "        elif(primeira_camada == 'L'): \n",
    "            \n",
    "            x = LSTM(numero_neuronios*proporcoes[0]//sum(proporcoes))(i)\n",
    "     \n",
    "        elif(primeira_camada == 'G'): \n",
    "            \n",
    "            x = GRU(numero_neuronios*proporcoes[0]//sum(proporcoes))(i)\n",
    "    \n",
    "        elif(primeira_camada == 'E'): \n",
    "            \n",
    "            x = RNN(ESNCell(numero_neuronios*proporcoes[0]//sum(proporcoes)))(i)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    x = Dense(1)(x)\n",
    "    \n",
    "    model = Model(i,x)\n",
    "    \n",
    "    model.compile(loss = 'mse', optimizer = Adam(learning_rate = 0.01))\n",
    "\n",
    "    return(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNÇÃO TREINADORA DE MODELO GENERALIZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Treinadora_modelo(model, X_treino, Y_treino, X_teste, Y_teste):\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20, restore_best_weights=True)\n",
    "    r = model.fit(X_treino, Y_treino, epochs = 100, validation_data=(X_teste, Y_teste), callbacks=[early_stop])\n",
    "    \n",
    "    return(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNÇÃO DE PREDIÇÕES DO MODELO GENERALIZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomar cuidado, na função a seguir, com o passo de atualização das janelas. Do jeito que o código está escrito, as janelas de\n",
    "# input DEVEM conter 4 features, sendo a última delas a Penicillin concentration(P:g/L). Caso o usuário queira alterar o número\n",
    "# de features, esta parte deve ser revisada.\n",
    "\n",
    "def Predicao_modelo(inputs_batch, outputs_batch, model):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    validation_target  = [0]*len(batches_teste)\n",
    "\n",
    "    final_predictions = []\n",
    "    \n",
    "    for k in range(len(batches_teste)):\n",
    "\n",
    "        validation_predictions = []\n",
    "        validation_target[k] = outputs_batch[k]\n",
    "\n",
    "        # first validation input\n",
    "        i = 0\n",
    "        last_x = inputs_batch[k][0]\n",
    "\n",
    "        while len(validation_predictions) < len(validation_target[k])-1:\n",
    "            p = model.predict(last_x.reshape(1, -1, qntd_features_input))[0,0] # O last_x é um array de ordem 2. Por isso, precisamos fazer o reshape para que seja de ordem 3\n",
    "\n",
    "            # update the predictions list\n",
    "            validation_predictions.append(p)\n",
    "\n",
    "            # make the new input\n",
    "            # fazer só para a penicilina \n",
    "\n",
    "            last_x = np.roll(last_x, -1, axis = 0)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "            last_x[-1][0] = inputs_batch[k][i][-1][0] # 'Carbon evolution rate(CER:g/h)'\n",
    "            last_x[-1][1] = inputs_batch[k][i][-1][1] # 'Dissolved oxygen concentration(DO2:mg/L)'\n",
    "            last_x[-1][2] = inputs_batch[k][i][-1][2] # 'Substrate concentration(S:g/L)'\n",
    "            last_x[-1][3] = p                         # 'Penicillin concentration(P:g/L)'\n",
    "\n",
    "\n",
    "        final_predictions.append(validation_predictions)\n",
    "\n",
    "        print('%d%%' %(100*((k+1)/len(batches_teste))), sep=' ', end='\\r')\n",
    "\n",
    "    print(\"\\n--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return(final_predictions, validation_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação dos modelos gerados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Avaliacao_modelos(batches_teste, validation_target, final_predictions):\n",
    "    #precisamos remover o ultimo elemento de cada validation_target[j]. Para evitar um erro anterior, tivemos de fazer cada\n",
    "    #final_predictions_geral[i][j] um elemento mais curto\n",
    "    i=0\n",
    "    dict_preds_multistep = {}\n",
    "    for batch in batches_teste:\n",
    "        dict_preds_multistep[batch] = final_predictions[i]\n",
    "        i+=1\n",
    "\n",
    "    i=0\n",
    "    dict_valid = {}\n",
    "    for batch in batches_teste:\n",
    "        dict_valid[batch] = validation_target[i]\n",
    "        i+=1\n",
    "\n",
    "    dict_rmse = {}\n",
    "    for batch in batches_teste:\n",
    "        dict_rmse[batch] = metrics.mean_squared_error(dict_preds_multistep[batch], dict_valid[batch][:-1], squared=False)\n",
    "    return (dict_rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registro dos resultados em um arquivo excel + plots em uma pasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Salva_resultados(descricao_modelo, r_replicas, final_predictions_replicas, dict_rmse_replicas, model_replicas, quantidade_replicas_por_modelo, modelo_idx):\n",
    "\n",
    "    # CRIA TODOS OS DATAFRAMES QUE PRECISAMOS:\n",
    "    \n",
    "    # Sheet 1: informações sobre batches de treino e de teste; features utilizados\n",
    "\n",
    "    resultados_modelo_sheet1 = {}\n",
    "\n",
    "    resultados_modelo_sheet1['batches_utilizados'] = [str(batches_utilizados)]\n",
    "    resultados_modelo_sheet1['batches_treino'] = [str(batches_treino)]\n",
    "    resultados_modelo_sheet1['número de batches no treino'] = [str(len(batches_treino))]\n",
    "    resultados_modelo_sheet1['batches_teste'] = [str(batches_teste)]\n",
    "    resultados_modelo_sheet1['número de batches no teste'] = [str(len(batches_teste))]\n",
    "    resultados_modelo_sheet1['número de dados (linhas) p/ cada batch'] = [str(len_batches)]\n",
    "    resultados_modelo_sheet1['número de horas de experimento p/ cada batch'] = [str(batches_list[batches_treino[0]]['Time (h)'].iloc[-1])]\n",
    "\n",
    "    resultados_modelo_sheet1['colunas_input'] = [str(colunas_input)]\n",
    "    resultados_modelo_sheet1['colunas_output'] = [str(colunas_output)]\n",
    "    resultados_modelo_sheet1['tamanho janela'] = [str(T)]\n",
    "\n",
    "    df_resultados_modelo_sheet1 = pd.DataFrame(resultados_modelo_sheet1).T\n",
    "\n",
    "    #------------\n",
    "    # Sheet 2: resumo da estrutura do modelo de rede neural\n",
    "\n",
    "    stringlist = []\n",
    "    y = model_replicas[0].summary(line_length=70, print_fn=lambda x: stringlist.append(x))   #fonte: https://stackoverflow.com/questions/65873433/get-keras-model-summary-as-table\n",
    "    df1_resultados_modelo_sheet2 = pd.DataFrame(stringlist, columns = ['modelo: ' + str(descricao_modelo)]).drop([3,10]) #dropei as linhas 3 e 10 ques estavam bugando o excel (não tinha nada de relevante nelas)\n",
    "\n",
    "    optimizer_config = model_replicas[0].optimizer.get_config()\n",
    "    parametros = [model_replicas[0].loss, optimizer_config['name'], optimizer_config['learning_rate'], len(r_replicas[0].history['loss']) ] #fonte: https://stackoverflow.com/questions/60212925/is-there-a-keras-function-to-obtain-the-compile-options\n",
    "    df2_resultados_modelo_sheet2 = pd.DataFrame(parametros, ['Loss function', 'Optimizer', 'Learning rate', 'Epochs efetuadas (máx: 100)'])\n",
    "\n",
    "    #------------\n",
    "    # Sheet 3: detalhes de cada layer do modelo de rede neural (input; hidden layers; output)\n",
    "\n",
    "    qtde_layers = len(model_replicas[0].layers)\n",
    "    detalhes_layers = [0]*(qtde_layers)\n",
    "    for i in range(qtde_layers):\n",
    "        configs_layer = pd.DataFrame(np.array([[str(x) for x in model_replicas[0].layers[i].get_config().values()]]),\n",
    "                                     columns=[str(x) for x in model_replicas[0].layers[i].get_config().keys()], index = ['layer'+str(i)]).T\n",
    "        detalhes_layers[i] = configs_layer\n",
    "    \n",
    "    #------------\n",
    "    # Sheet 4: loss e val_loss\n",
    "    \n",
    "    df_loss = [0]*(quantidade_replicas_por_modelo)\n",
    "    df_val_loss = [0]*(quantidade_replicas_por_modelo)\n",
    "    \n",
    "    for i in range(quantidade_replicas_por_modelo):\n",
    "        df_loss[i] = pd.DataFrame(r_replicas[i].history['loss'], columns = ['loss replica ' + str(i+1)])\n",
    "        df_val_loss[i] = pd.DataFrame(r_replicas[i].history['val_loss'], columns = ['val_loss replica ' + str(i+1)])\n",
    "\n",
    "    #------------\n",
    "    # Sheet 5: resultados das predições one-step\n",
    "    \n",
    "    df_Y_treino = pd.DataFrame(Y_treino.flatten(), columns = ['Y_treino'])\n",
    "    df_predicoes_onestep_treino_replicas = [0]*(quantidade_replicas_por_modelo)\n",
    "    for i in range(quantidade_replicas_por_modelo):\n",
    "        df_predicoes_onestep_treino_replicas[i] = pd.DataFrame(model_replicas[i].predict(X_treino).flatten(), columns = ['predicoes treino one-step replica ' + str(i+1)])\n",
    "    \n",
    "    df_Y_teste = pd.DataFrame(Y_teste.flatten(), columns = ['Y_teste'])\n",
    "    df_predicoes_onestep_teste_replicas = [0]*(quantidade_replicas_por_modelo)\n",
    "    for i in range(quantidade_replicas_por_modelo):\n",
    "        df_predicoes_onestep_teste_replicas[i] = pd.DataFrame(model_replicas[i].predict(X_teste).flatten(), columns = ['predicoes teste one-step replica ' + str(i+1)])\n",
    "\n",
    "    #------------\n",
    "    # Sheet 6: resultados das predições multi-step\n",
    "    \n",
    "    df_predicoes_multistep_replicas = [0]*(quantidade_replicas_por_modelo)\n",
    "    \n",
    "    for i in range(quantidade_replicas_por_modelo):\n",
    "        \n",
    "        df_predicoes_multistep_batch = [0]*(len(batches_teste))\n",
    "        \n",
    "        passador = 0\n",
    "        \n",
    "        for j in range(len(batches_teste)):\n",
    "            \n",
    "            df_predicoes_multistep_batch[passador] = pd.DataFrame(np.array(final_predictions_replicas[i][j]).flatten(),  columns = ['predicoes multi-step replica ' + str(i+1) + ', batch ' + str(batches_teste[j])])\n",
    "            passador += 1\n",
    "            \n",
    "        df_predicoes_multistep_replicas[i] = df_predicoes_multistep_batch\n",
    "    \n",
    "    #------------\n",
    "    # Sheet 7: RMSEs\n",
    "    \n",
    "    df_rmses_replicas = [0]*(quantidade_replicas_por_modelo)\n",
    "    \n",
    "    for i in range(quantidade_replicas_por_modelo):\n",
    "        rmses_replica = dict_rmse_replicas[i]   \n",
    "        df_rmses = pd.DataFrame(rmses_replica, index = ['replica ' + str(i+1)])\n",
    "        df_rmses['média'] = df_rmses.T.mean()\n",
    "        df_rmses = df_rmses.T\n",
    "        \n",
    "        df_rmses_replicas[i] = df_rmses\n",
    "        \n",
    "        del df_rmses\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    #SALVA TODOS OS DATAFRAMES EM UM EXCEL\n",
    "    \n",
    "    #path_pasta = \"Modelo\"+str(modelo_idx)+'_'+descricao_modelo[0]+'_'+descricao_modelo[1]+'_'+descricao_modelo[2]+'_'+str(descricao_modelo[3]) \n",
    "    #os.mkdir(path_pasta)\n",
    "    \n",
    "    with pd.ExcelWriter(\"Modelo\"+str(modelo_idx)+'_'+descricao_modelo[0]+'_'+descricao_modelo[1]+'_'+descricao_modelo[2]+'_'+str(descricao_modelo[3])+'_resultados.xlsx', engine=\"openpyxl\") as writer:\n",
    "        #, if_sheet_exists=\"overlay\"\n",
    "        #Sheet 1\n",
    "        df_resultados_modelo_sheet1.to_excel(writer, sheet_name=\"Info inputs\")\n",
    "\n",
    "        #Sheet 2\n",
    "        df1_resultados_modelo_sheet2.to_excel(writer, sheet_name=\"Resumo modelo\")  \n",
    "        df2_resultados_modelo_sheet2.to_excel(writer, sheet_name=\"Resumo modelo\", startrow=16) \n",
    "\n",
    "        #Sheet 3\n",
    "        for i in range(qtde_layers):\n",
    "            detalhes_layers[i].to_excel(writer, sheet_name=\"Detalhes layers\", startcol=i*4) \n",
    "\n",
    "        #Sheet 4\n",
    "        for i in range(quantidade_replicas_por_modelo):\n",
    "            df_loss[i].to_excel(writer, sheet_name=\"loss e val_loss\", startcol = i*6) \n",
    "            df_val_loss[i].to_excel(writer, sheet_name=\"loss e val_loss\", startcol = (i*6)+3) \n",
    "\n",
    "        #Sheet 5\n",
    "        df_Y_treino.to_excel(writer, sheet_name=\"predicoes one-step\", startcol = 0) \n",
    "        for i in range(quantidade_replicas_por_modelo):\n",
    "            df_predicoes_onestep_treino_replicas[i].to_excel(writer, sheet_name=\"predicoes one-step\", startcol = (i+1)*3)     \n",
    "\n",
    "        df_Y_teste.to_excel(writer, sheet_name=\"predicoes one-step\", startcol = (quantidade_replicas_por_modelo+1)*3) \n",
    "        for i in range(quantidade_replicas_por_modelo):\n",
    "            df_predicoes_onestep_teste_replicas[i].to_excel(writer, sheet_name=\"predicoes one-step\", startcol = (quantidade_replicas_por_modelo+i+2)*3)     \n",
    "\n",
    "        #Sheet 6\n",
    "        for i in range(quantidade_replicas_por_modelo):\n",
    "\n",
    "            for j in range(len(batches_teste)):\n",
    "\n",
    "                df_predicoes_multistep_replicas[i][j].to_excel(writer, sheet_name=\"predicoes multi-step\", startcol = (3*i*len(batches_teste) + j*3))     \n",
    "\n",
    "        #Sheet 7\n",
    "        for i in range(quantidade_replicas_por_modelo):\n",
    "            df_rmses_replicas[i].to_excel(writer, sheet_name=\"RMSEs\", startcol = (i+1)*3)     \n",
    "\n",
    "\n",
    "    #fonte: https://pandas.pydata.org/docs/reference/api/pandas.ExcelWriter.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computa-se todos os modelos requeridos - tomando atenção para respeitar as convenções criadas abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recortar em cada batch para fazer a retroalimentação\n",
    "# len_batches-T #número de janelas em cada batch\n",
    "\n",
    "N_inputs_batch = len_batches-T\n",
    "\n",
    "primeiro_input = 0\n",
    "\n",
    "#Cada elemento de inputs_batch e de outputs_batch será composto de uma matriz ou lista que contém todos os inputs ou outputs\n",
    "#de determinado batch. Estas variáveis servem apenas para separarmos batch por batch.\n",
    "inputs_batch = []\n",
    "outputs_batch = []\n",
    "\n",
    "for i in range(len(batches_teste)):\n",
    "    \n",
    "    inputs_batch.append(X_teste[primeiro_input:primeiro_input+N_inputs_batch])\n",
    "    outputs_batch.append(Y_teste[primeiro_input:primeiro_input+N_inputs_batch])\n",
    "\n",
    "    primeiro_input += N_inputs_batch\n",
    "    \n",
    "tipos_camadas = ['0', 'S', 'L', 'G', 'E']\n",
    "combinacoes_camadas = [[1,1,1], [2,2,2], [3,3,3], [4,4,4], \n",
    "                       [1,1,0], [2,2,0], [3,3,0], [4,4,0], \n",
    "                       [1,0,0], [2,0,0], [3,0,0], [4,0,0]]\n",
    "                      \n",
    "      \n",
    "#modelos = [ ['D', '0', '0', 1] ]  # Lista dos modelos que serão testados\n",
    "\n",
    "modelos = []\n",
    "num_tot_neuronios = 30\n",
    "\n",
    "for i in range(len(combinacoes_camadas)):\n",
    "    modelos.append([tipos_camadas[combinacoes_camadas[i][0]], tipos_camadas[combinacoes_camadas[i][1]], tipos_camadas[combinacoes_camadas[i][2]], num_tot_neuronios]) \n",
    "\n",
    "# Cada elemento do objeto \"modelos\" é a descrição da estrutura do modelo a ser executado. Esta descrição obedece a seguinte\n",
    "# sequência: [primeira_camada, segunda_camada, terceira_camada, numero_neuronios]. O numero_neuronios é o número total de\n",
    "# neurônios utilizados na rede neural. As camadas podem ser \"S\", \"L\", \"G\" ou \"E\", representado cada RNN estudada, ou, ainda,\n",
    "# \"0\", representando a ausência de determinada camada na rede neural. Ex: [2,2,0,30] trata de uma rede neural composta de \n",
    "# duas camadas do tipo LSTM, e que contém 30 neurônios ao todo. Lembrando que a distribuição de neurônios por camada é de 3:2:1\n",
    "# quando há 3 camadas, e 2:1 quando há 2 camadas.\n",
    "    \n",
    "quantidade_replicas_por_modelo = 3\n",
    "modelos_idx = [m for m in range(12)]\n",
    "passador_modelos = 0\n",
    "\n",
    "for i in modelos:\n",
    "\n",
    "    primeira_camada, segunda_camada, terceira_camada, numero_neuronios = i[0], i[1], i[2], i[3]\n",
    "\n",
    "    #Guardarei o 'r', 'final_predictions' e 'dict_rmse' de cada réplica para poder colocar tudo em um só arquivo excel\n",
    "    model_replicas = [0]*(quantidade_replicas_por_modelo)  # Só preciso desse model_replicas para poder colocar as predições one-step de cada modelo no excel\n",
    "    r_replicas = [0]*(quantidade_replicas_por_modelo)\n",
    "    final_predictions_replicas = [0]*(quantidade_replicas_por_modelo)\n",
    "    dict_rmse_replicas = [0]*(quantidade_replicas_por_modelo)\n",
    "\n",
    "    for j in range(quantidade_replicas_por_modelo):\n",
    "        set_seed(j)\n",
    "        \n",
    "        #Cria o modelo\n",
    "        model_replicas[j] = Modelo_generalizado(primeira_camada, segunda_camada, terceira_camada, numero_neuronios, T, qntd_features_input)\n",
    "        \n",
    "        r_replicas[j] = Treinadora_modelo(model_replicas[j], X_treino, Y_treino, X_teste, Y_teste)\n",
    "        \n",
    "        print('acabou r_replicas')\n",
    "        final_predictions_replicas[j], validation_target = Predicao_modelo(inputs_batch, outputs_batch, model_replicas[j])\n",
    "\n",
    "        dict_rmse_replicas[j] = Avaliacao_modelos(batches_teste, validation_target, final_predictions_replicas[j])\n",
    "        \n",
    "    modelo_idx = modelos_idx[passador_modelos]\n",
    "    Salva_resultados(i, r_replicas, final_predictions_replicas, dict_rmse_replicas, model_replicas, quantidade_replicas_por_modelo, modelo_idx)\n",
    "\n",
    "    passador_modelos += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "10.4_TCC_Pen_Explor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
